{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9635dc1",
   "metadata": {},
   "source": [
    "# Example of EMM for machine learning reweighting using ADMM optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a97c3c6",
   "metadata": {},
   "source": [
    "Using the PIMA diabetes dataset we compare the original dataset with labels to the reweighted dataset with artificial labels. We begin by importing the PIMA diabetes dataset, it is already cleaned and does not have any missing values, outliers, etc. This example is also nice since all features are continuous and the labels are binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a94bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data, convert to csv, and import\n",
    "# Update directory\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "# Import libraries\n",
    "from emm import *\n",
    "import os.path\n",
    "from get_data import get_data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Data locations\n",
    "raw_dir = \"../data/raw/pima_diabetes/archive.zip\"\n",
    "processed_dir = \"../data/processed/pima_diabetes/\"\n",
    "\n",
    "# If processed data does exist convert raw to csv\n",
    "if not os.path.exists(processed_dir + \"diabetes.csv\"):\n",
    "    get_data.RData_to_csv(raw_dir,processed_dir)\n",
    "    \n",
    "# Get data into dataframe\n",
    "df = pd.read_csv(processed_dir + \"diabetes.csv\")\n",
    "\n",
    "# Replace 0 with nan\n",
    "nan_cols = ['Glucose', 'BloodPressure','SkinThickness','Insulin','BMI']\n",
    "df[nan_cols]=df[nan_cols].replace({'0':np.nan,0:np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42042dc4",
   "metadata": {},
   "source": [
    "Separating features from labels, useful for training ML models and for re-weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d779c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_splitting of the dataset\n",
    "X = df.drop(columns = 'Outcome')\n",
    "# Getting Predicting Value\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fabb8a",
   "metadata": {},
   "source": [
    "First, we look at the means for each features according to outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede99a18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "marginals_mean = df.groupby('Outcome').mean()\n",
    "display(marginals_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176f6c6",
   "metadata": {},
   "source": [
    "We may also want to consider standard deviation of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginals_std = df.groupby('Outcome').std()\n",
    "display(marginals_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8fb72",
   "metadata": {},
   "source": [
    "### Matching marginals\n",
    "\n",
    "First we construct functions $F$ upon which we take expectations of under the weighted measure. Mathematically, we express this as\n",
    "$$ \\mathbb E[F(x)] = \\sum^N_{i=1} w_i F(x_i) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(X)\n",
    "marginals = {}\n",
    "for feature in features:\n",
    "    marginals[feature] = ['mean', 'std']\n",
    "marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de102a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_0 = []\n",
    "loss_1 = []\n",
    "for feature in marginals.keys():\n",
    "    for fun in marginals[feature]:\n",
    "        marg = getattr(df[[feature,'Outcome']].groupby('Outcome'),fun)()\n",
    "        loss_0.append(EqualityLoss(marg.loc[0]))\n",
    "        loss_1.append(EqualityLoss(marg.loc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca52ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regularizer = regularizers.EntropyRegularizer(limit=None)\n",
    "#regularizer = regularizers.ZeroRegularizer()\n",
    "w_0, out_0, sol_0 = emm(X, marginals, loss_0, regularizer, \n",
    "                        optimizer = 'gurobi', verbose=False, rho=25, \n",
    "                        eps_abs=1e-6, eps_rel=1e-6)\n",
    "w_1, out_1, sol_1 = emm(X, marginals, loss_1, regularizer,\n",
    "                     optimizer='gurobi', verbose=False, rho=24,\n",
    "                        eps_abs=1e-6, eps_rel=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da33a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = X.copy()\n",
    "X_0[\"weights\"] = w_0 \n",
    "X_1 = X.copy()\n",
    "X_1[\"weights\"] = w_1 \n",
    "\n",
    "# Set theoretical outcome to train on reweighted datasets\n",
    "X_0['Outcome'] = 0\n",
    "X_1['Outcome'] = 1\n",
    "\n",
    "df_w = pd.concat([X_0,X_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810af26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valw = df_w.drop(columns=['Outcome','weights']).multiply(df_w['weights'], axis=\"index\")\n",
    "df_valw['Outcome'] = df_w['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_marginals = df_valw.groupby('Outcome').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461351fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(marginals_mean)\n",
    "display(w_marginals)\n",
    "display(abs((marginals_mean - w_marginals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e084879",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_w = pd.concat([abs(X_0[features] - marginals_mean.loc[0]),abs(X_1[features] - marginals_mean.loc[1])])\n",
    "std_w = std_w.multiply(df_w['weights'], axis=\"index\")\n",
    "std_w = pd.concat([std_w, df_w['Outcome']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(marginals_std)\n",
    "display(std_w.groupby('Outcome').sum())\n",
    "display(abs((marginals_std - std_w.groupby('Outcome').sum())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
